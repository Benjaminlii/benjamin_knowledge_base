# Note Of Read Kafka Docs

## 1. 开始

### 1.1 引言

> https://kafka.apache.org/documentation/#introduction

#### 1.1.1什么是事件流?

事件流相当于数字版的人体中枢神经系统.

在未来, 业务将越来越多的由软件定义, 且自动化, 并且软件的用户也是软件.事件流是实现这一目标的技术基础.

从技术上讲, 事件流是实时的从事件源中捕获数据的一种实践, 存储这些数据供后续的检索, 实时地回顾性地对事件流进行操控、处理以及响应. 根据需要讲事件流路由到不同的目标技术中. 因此事件流保证了数据的连续流动和解析, 从而让正确的信息出现在正确的时间和正确的地点.

#### 1.1.2我们可以使用事件流来做什么?

事件流被广泛的应用与大量的行业和组织的各种各样的用例中. 举例:

1. 实时的处理支付和金融事务, 如股票交易、银行和保险业.
1. 实时的跟踪和监控汽车、车队以及货运, 如物流和汽车行业.
1. 持续的捕获和分析来自物联网设备或者其他设备中的传感器数据.
1. 收集并立即响应用户的活动以及指令.
1. 连接、存储和提供由公司不同部门产生的数据.
1. 是数据平台, 事件驱动架构以及微服务的基础.

#### 1.1.3Kafka是一个事件流平台, 意味着什么?

Kafka提供了三种能力来实现端到端的事件流用例:

1. 发布和订阅事件流
1. 持久并且可靠的存储事件流
1. 事件发生时或者回溯性的处理事件流

所有这些功能都以分布式, 可拓展, 伸缩, 容错, 安全的方式提供. Kafka可以部署在物理机, 虚拟机, 容器, 本机以及云中.你可以自己管理Kafka环境也可以使用提供商提供的完整托管服务.

#### 1.1.4Kafka是如何工作的

Kafka是服务端和客户端通过高性能TCP协议通信的一个分布式系统。可以部署在物理机, 虚拟机, 容器中。

服务端：Kafka作为集群运行, 可以扩越多个区域。其中一些服务构成存储层, 成为代理。其他的服务器运行Kafka connect, 通过事件流的形式导入导出数据, 从而将Kafka和现有的系统（如关系型数据库）集成。Kafka有高度可扩展行和容错性, 如何任何一个服务器出现错误, 其他的服务器会代替它工作。

客户端：允许你便携分布式的app和微服务, 在并行, 大规模和容错的方式读写和处理事件流。Kafka附带了一些客户端, 在java, scala的客户端中可以支持更高级的 Kafka Streams 库。还提供了go, python, c/c++等等编程语言的客户端以及REST API。

#### 1.1.5主要概念和术语

一个事件意味着你的业务中“有什么事情发生了”。这在文档中也被称为记录或者消息。当你在Kafka中读写数据时，你就用事件的形式做了这件事。概念上，一个事件包含键，值，时间戳和可选的元数据头。举例如下：

- 键：Alice
- 值：付200给Bob
- 时间戳：2022.1.25 14:06

生产者是发布时间到Kafka的这些客户端应用，消费者是订阅（读并且处理）这些事件的端。在Kafka中，生产者和消费者是完全解耦并且相互不感知的。这是Kafka实现总所周知的改读可拓展性的关键设计元素。举个例子，生产者从不需要等待消费者。Kafka提供了各种各样的保证，比如只处理一次事件的能力。

事件是有组织的并且持久的存储在主题（topic）中。非常简单，一个主题就是一个文件系统中的文件夹，事件则是这个文件夹中的文件。主题在Kafka中总是多生产者和多订阅的：一个主题可以有零至多个生产者想起中写入事件，也可以有零至多个消费者订阅这些事件。主题中的事件可以根据需要随时读取，这和传统消息系统不同，事件在在被消费后不会被删除。相反，你可以定义Kafka按照主题纬度定义保留你的事件多久，在此之后，旧的事件将会被丢弃。Kafka的性能在数据大小方面是有效恒定的，所以长时间的存储数据时完全没问题的。

主题是分段的，这意味着主题分布在不同Kafka代理的许多“桶”上。这种分布式的数据放置对可拓展性是非常重要的，因为这允许客户端应用同时在多个代理中读写数据。当新事件被发布到主题中时，实际上会被附加到这个主题的某个分区中。具有相同事件key的事件会被写入到同一个分区中，Kafka保证指定主题分区的多个消费者总是在用事件写入的顺序来读取该分区的事件。

![主题分区](https://kafka.apache.org/images/streams-and-tables-p1_p4.png)
上图：示例的主题有四个分区P1-P4，两个不同的生产者相互独立地通过网络向主题的分区中写入事件来发布新事件。这些事件拥有同样的key（图中按颜色表示）的会被写入到同一个分区中。两个生产者的事件也可以被写入同一个分区，只要key相同。

为了数据的容错性和高可用性，每一个主题都可以被复制，甚至跨越地理区域或者数据中心，所以总会有多个代理拥有拷贝的数据，以防止出现问题，你可以去维护代理等等。一个公用的生产设置是复制因子为3，也就是说，一份数据总会有三个副本。这种复制是在主题分区级别执行的。


#### 1.1.6 Kafka APIs
除了命令行工具之外，Kafka还为Java和Scala提供了5个核心API：
- 管理api用于管理和检查主题，代理和其他的Kafka实例。
- 生产者api用于向一个或者多个Kafka主题发布时间流消息。
- 消费者api用于订阅一至多个主题的消息并进行处理。
- Kafka流api实现流处理应用和微服务。它提供了高级方法来处理事件流，包括转换，操作状态（聚合和连接），窗口，基于事件时间的处理等等。从一至多个主题中读取输入，为一至多个主题生成输出。有效的实现输入输出的转换。
- Kafka连接api可以构建和运行可复用的导入导出连接器，这些连接器可以让外部系统或应用向生产者或者消费者中读写数据，从而让他们与Kafka集成。例如：一个连接到关系型数据库的连接器可以捕获对表的每一次更改。实际上并不需要实现自己的连接器，Kafka社区已经提供了非常多现成的连接器。

### 1.2 用例
以下是关于Kafka的一些流行用例：

#### 1.2.1消息
Kafka可以非常好的替代传统的消息代理。使用消息代理的原因有很多（从数据生产者那里解耦消息的处理，缓存未处理的消息，等等）。相对于大多数的消息系统，Kafka拥有更好的吞吐量，内置的分区，复制和容错，这使得Kafka成为大规模数据处理应用的一个好的解决方案。
在我们的经验里，消息的使用通常是低吞吐量的，但可能会需要低的端到端延迟，并且通常依赖Kafka提供的强大的持久化保证。

在这个领域内kafka可以于传统的消息系统相媲美。

#### 1.2.2网站活动跟踪
Kafka的原始用例是能够重构用户活动跟踪流水线为一组实时的发布订阅数据源。这意味着网站活动（页面浏览，搜索或者其他用户可能的活动）会被发布到核心主题，每一个活动类型只有一个主题。这些数据源可以用于订阅一些用例，包括实时处理，实时监控，以及加载到Hadoop或者离线的数据仓库系统以供离线处理和报告。
活动跟踪的量通常非常大，每个用户页面都会生成很多活动消息。

#### 1.2.3指标
Kafka常常被用来操作监控数据。这涉及到聚合来自分布式应用的统计信息，并生成集中式的操作数据源。

#### 1.2.4日志聚合
很多人将Kafka作为日志聚合解决方案的替代品。日志聚合通常需要在服务器外收集原始日志文件，并且将它们上传到一个统一的位置（一个文件服务或者Hadoop分布式文件系统）去处理。Kafka抽离了文件细节，并且以更清晰的方式将日志或者事件数据抽象为一系列消息。这个允许更低延迟的处理、更容易支持的多数据源和分布式数据消费。相对比Scribe或者Flume这样的以日志为中心的系统，Kafka提供了同样好的性能，由于复制带来的更强大的持久化保证，更低的端到端延迟。
